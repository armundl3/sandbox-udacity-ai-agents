{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Tools - Fixed Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import (\n",
    "    HumanMessage, \n",
    "    SystemMessage, \n",
    "    ToolMessage\n",
    ")\n",
    "from langchain.tools import tool\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def plot_beta_distribution(alpha: float, beta_param: float, n_points: int = 1000) -> str:\n",
    "    \"\"\"\n",
    "    Plots the Beta distribution for given alpha and beta parameters.\n",
    "    Use this when asked to plot, visualize, or show a beta distribution.\n",
    "    \n",
    "    Args:\n",
    "        alpha (float): Shape parameter α (> 0)\n",
    "        beta_param (float): Shape parameter β (> 0)\n",
    "        n_points (int): Number of points in the x-axis grid\n",
    "    \n",
    "    Returns:\n",
    "        str: Confirmation message\n",
    "    \"\"\"\n",
    "    # Create x values between 0 and 1\n",
    "    x = np.linspace(0, 1, n_points)\n",
    "    \n",
    "    # Compute the Beta PDF for each x\n",
    "    y = beta.pdf(x, alpha, beta_param)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(x, y, 'b-', lw=2, label=f'Beta(α={alpha}, β={beta_param})')\n",
    "    plt.title(\"Beta Distribution\", fontsize=14)\n",
    "    plt.xlabel(\"x\", fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return f\"Successfully plotted Beta distribution with α={alpha}, β={beta_param}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [multiply, plot_beta_distribution]\n",
    "tool_map = {tool.name: tool for tool in tools}\n",
    "print(f\"Available tools: {list(tool_map.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bind Tools to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tool_choice=\"any\" to force the LLM to always use tools\n",
    "llm_with_tools = llm.bind_tools(tools, tool_choice=\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Multiply Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# RESET: Create fresh messages for this example\nquestion = \"What is 3 multiplied by 2?\"\nmessages = [\n    SystemMessage(\"You're a helpful assistant. Use tools when available.\"),\n    HumanMessage(question)\n]\n\nprint(f\"Question: {question}\")\nprint()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: LLM decides to call a tool\n",
    "ai_message = llm_with_tools.invoke(messages)\n",
    "\n",
    "print(\"LLM Response:\")\n",
    "print(f\"Content: '{ai_message.content}'\")\n",
    "print(f\"Tool calls: {ai_message.tool_calls}\")\n",
    "print()\n",
    "\n",
    "# Add to message history\n",
    "messages.append(ai_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Execute the tool(s)\n",
    "if ai_message.tool_calls:\n",
    "    for tool_call in ai_message.tool_calls:\n",
    "        tool_call_id = tool_call['id']\n",
    "        function_name = tool_call['name']\n",
    "        arguments = tool_call['args']\n",
    "        \n",
    "        print(f\"Executing: {function_name}({arguments})\")\n",
    "        \n",
    "        # Execute the tool\n",
    "        func = tool_map[function_name]\n",
    "        result = func.invoke(arguments)\n",
    "        \n",
    "        print(f\"Result: {result}\")\n",
    "        print()\n",
    "        \n",
    "        # Create tool message with result\n",
    "        tool_message = ToolMessage(\n",
    "            content=str(result),\n",
    "            name=function_name,\n",
    "            tool_call_id=tool_call_id,\n",
    "        )\n",
    "        messages.append(tool_message)\n",
    "else:\n",
    "    print(\"No tool calls made.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Get final formatted response from LLM\n",
    "final_response = llm_with_tools.invoke(messages)\n",
    "\n",
    "print(\"Final Answer:\")\n",
    "print(final_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Plot Beta Distribution Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# RESET: Create fresh messages for this example\nquestion = \"Plot a beta distribution with alpha=2 and beta=5\"\nmessages = [\n    SystemMessage(\"You're a helpful assistant. Use tools when available.\"),\n    HumanMessage(question)\n]\n\nprint(f\"Question: {question}\")\nprint()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: LLM decides to call a tool\n",
    "ai_message = llm_with_tools.invoke(messages)\n",
    "\n",
    "print(\"LLM Response:\")\n",
    "print(f\"Content: '{ai_message.content}'\")\n",
    "print(f\"Tool calls: {ai_message.tool_calls}\")\n",
    "print()\n",
    "\n",
    "# Add to message history\n",
    "messages.append(ai_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Execute the tool(s)\n",
    "if ai_message.tool_calls:\n",
    "    for tool_call in ai_message.tool_calls:\n",
    "        tool_call_id = tool_call['id']\n",
    "        function_name = tool_call['name']\n",
    "        arguments = tool_call['args']\n",
    "        \n",
    "        print(f\"Executing: {function_name}({arguments})\")\n",
    "        print()\n",
    "        \n",
    "        # Execute the tool (this will display the plot)\n",
    "        func = tool_map[function_name]\n",
    "        result = func.invoke(arguments)\n",
    "        \n",
    "        print(f\"Result: {result}\")\n",
    "        print()\n",
    "        \n",
    "        # Create tool message with result\n",
    "        tool_message = ToolMessage(\n",
    "            content=str(result),\n",
    "            name=function_name,\n",
    "            tool_call_id=tool_call_id,\n",
    "        )\n",
    "        messages.append(tool_message)\n",
    "else:\n",
    "    print(\"No tool calls made.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Get final formatted response from LLM\n",
    "final_response = llm_with_tools.invoke(messages)\n",
    "\n",
    "print(\"Final Answer:\")\n",
    "print(final_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Complete Tool Calling Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_tools(question: str, llm_with_tools, tool_map, verbose=True):\n",
    "    \"\"\"\n",
    "    Complete tool calling flow:\n",
    "    1. LLM decides which tool to call\n",
    "    2. Execute the tool\n",
    "    3. LLM formats final answer\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(\"You're a helpful assistant. Use tools when available.\"),\n",
    "        HumanMessage(question)\n",
    "    ]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Question: {question}\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: LLM decides to call tools\n",
    "    ai_message = llm_with_tools.invoke(messages)\n",
    "    messages.append(ai_message)\n",
    "    \n",
    "    if verbose and ai_message.tool_calls:\n",
    "        print(f\"\\nLLM wants to call {len(ai_message.tool_calls)} tool(s)\")\n",
    "    \n",
    "    # Step 2: Execute tools\n",
    "    if ai_message.tool_calls:\n",
    "        for tool_call in ai_message.tool_calls:\n",
    "            function_name = tool_call['name']\n",
    "            arguments = tool_call['args']\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\nExecuting: {function_name}({arguments})\")\n",
    "            \n",
    "            # Execute the tool\n",
    "            func = tool_map[function_name]\n",
    "            result = func.invoke(arguments)\n",
    "            \n",
    "            # Add result to messages\n",
    "            tool_message = ToolMessage(\n",
    "                content=str(result),\n",
    "                name=function_name,\n",
    "                tool_call_id=tool_call['id'],\n",
    "            )\n",
    "            messages.append(tool_message)\n",
    "    \n",
    "    # Step 3: Get final answer\n",
    "    final_response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Final Answer:\")\n",
    "        print(final_response.content)\n",
    "    \n",
    "    return final_response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiplication\n",
    "ask_with_tools(\n",
    "    \"What is 7 times 8?\",\n",
    "    llm_with_tools,\n",
    "    tool_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test plotting\n",
    "ask_with_tools(\n",
    "    \"Show me a beta distribution with alpha=5 and beta=2\",\n",
    "    llm_with_tools,\n",
    "    tool_map\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\n1. **Tool calls are in `ai_message.tool_calls`** - Don't use `additional_kwargs`\n2. **Use `tool_choice=\"any\"`** to force tool usage\n3. **Two LLM invocations needed**:\n   - First: LLM decides which tool to call\n   - Second: LLM formats the final answer\n4. **Tools should return strings** for easy integration with `ToolMessage`\n5. **Message flow**: SystemMessage → HumanMessage → AIMessage (tool call) → ToolMessage (result) → AIMessage (final answer)\n\n## Troubleshooting\n\n**Error: \"An assistant message with 'tool_calls' must be followed by tool messages\"**\n\nThis means you have a tool call in your message history that was never executed. \n\n**Cause:** Cells were run out of order.\n\n**Fix:** Re-run the \"RESET\" cell (the first cell in each example) to create fresh messages, then run cells in order:\n1. RESET cell (creates messages)\n2. Step 1: LLM decides tool\n3. Step 2: Execute tool\n4. Step 3: Get final answer"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}