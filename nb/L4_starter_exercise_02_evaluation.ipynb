{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - Knowledge Base Agent - STARTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, youâ€™ll implement and evaluate a RAG (Retrieval-Augmented Generation) pipeline, using RAGAS metrics and MLflow for logging the process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to create a LangGraph Workflow that includes:\n",
    "\n",
    "- A RAG pipeline for information retrieval.\n",
    "- An LLM-based judge for evaluation.\n",
    "- RAGAS metrics for quality assessment.\n",
    "- MLflow logging for observability.\n",
    "\n",
    "The workflow should:\n",
    "\n",
    "- Retrieve, augment, and generate answers.\n",
    "- Evaluate the answers using RAGAS.\n",
    "- Log performance metrics in MLflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import the necessary libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from mlflow import log_params, log_metrics\n",
    "from typing import List, Dict\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.graph.message import MessagesState\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instantiate Chat Model with your API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to connect with OpenAI, you need to instantiate an ChatOpenAI client passing your OpenAI key.\n",
    "\n",
    "You can pass the `api_key` argument directly.\n",
    "```python\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    "    api_key=\"voc-\",\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Instantiate your chat model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    "    base_url=\"https://openai.vocareum.com/v1\",\n",
    "    api_key=os.getenv(\"VOCAREUM_OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Instantiate your llm as judge model\n",
    "# This will evaluate the responses\n",
    "llm_judge = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.0,\n",
    "    base_url=\"https://openai.vocareum.com/v1\",\n",
    "    api_key=os.getenv(\"VOCAREUM_OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Instantiate your embeddings model\n",
    "embeddings_fn = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    base_url=\"https://openai.vocareum.com/v1\",\n",
    "    api_key=os.getenv(\"VOCAREUM_OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"file:///Users/amundle/github/sandbox-udacity-ai-agents/mlruns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amundle/github/sandbox-udacity-ai-agents/.venv/lib/python3.13/site-packages/mlflow/tracking/_tracking_service/utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri, store_uri)\n",
      "2025/11/30 21:08:09 INFO mlflow.tracking.fluent: Experiment with name 'udacity' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///Users/amundle/github/sandbox-udacity-ai-agents/mlruns/465881862363542514', creation_time=1764558489927, experiment_id='465881862363542514', last_update_time=1764558489927, lifecycle_stage='active', name='udacity', tags={}>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"udacity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RunInfo: artifact_uri='file:///Users/amundle/github/sandbox-udacity-ai-agents/mlruns/465881862363542514/00669e73fec24bc08d720d1f21860d49/artifacts', end_time=None, experiment_id='465881862363542514', lifecycle_stage='active', run_id='00669e73fec24bc08d720d1f21860d49', run_name='l4_exercise_02', start_time=1764558489964, status='RUNNING', user_id='amundle'>\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"l4_exercise_02\") as run:\n",
    "    log_params(\n",
    "        {\n",
    "            \"embeddings_model\":embeddings_fn.model,\n",
    "            \"llm_model\": llm.model_name,\n",
    "            \"llm_judge_model\": llm_judge.model_name,\n",
    "        }\n",
    "    )\n",
    "    print(run.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_run_id = run.info.run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mflow_client = mlflow.tracking.MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Run: data=<RunData: metrics={}, params={'embeddings_model': 'text-embedding-3-large',\n",
       " 'llm_judge_model': 'gpt-4o',\n",
       " 'llm_model': 'gpt-4o-mini'}, tags={'mlflow.runName': 'l4_exercise_02',\n",
       " 'mlflow.source.name': '/Users/amundle/github/sandbox-udacity-ai-agents/.venv/lib/python3.13/site-packages/ipykernel_launcher.py',\n",
       " 'mlflow.source.type': 'LOCAL',\n",
       " 'mlflow.user': 'amundle'}>, info=<RunInfo: artifact_uri='file:///Users/amundle/github/sandbox-udacity-ai-agents/mlruns/465881862363542514/00669e73fec24bc08d720d1f21860d49/artifacts', end_time=1764558489968, experiment_id='465881862363542514', lifecycle_stage='active', run_id='00669e73fec24bc08d720d1f21860d49', run_name='l4_exercise_02', start_time=1764558489964, status='FINISHED', user_id='amundle'>, inputs=<RunInputs: dataset_inputs=[], model_inputs=[]>, outputs=<RunOutputs: model_outputs=[]>>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mflow_client.get_run(mlflow_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Process Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kl/80f0ft9x75q2x93y94lfw8800000gn/T/ipykernel_15106/1949086182.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(\n"
     ]
    }
   ],
   "source": [
    "# Initialize vector store\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"udacity\",\n",
    "    embedding_function=embeddings_fn,\n",
    "    api_key=os.getenv(\"VOCAREUM_OPENAI_API_KEY\"),\n",
    "    base_url=\"https://openai.vocareum.com/v1\",\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "# Load and process PDF documents\n",
    "file_path = \"compact-guide-to-large-language-models.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "pages = []\n",
    "for page in loader.load():\n",
    "    pages.append(page)\n",
    "\n",
    "# Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "all_splits = text_splitter.split_documents(pages)\n",
    "\n",
    "# Store document chunks in the vector database\n",
    "_ = vector_store.add_documents(documents=all_splits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define State Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a State Schema for managing:\n",
    "\n",
    "- MLFlow Run id\n",
    "- User query\n",
    "- Ground Truth\n",
    "- Retrieved documents\n",
    "- Generated answer\n",
    "- Evaluation Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run_id(str), ground_truth(str), evaluation(Dict),vquestion(str), documents(List) and answer(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Create your state schema\n",
    "# run_id(str), ground_truth(str), evaluation(Dict),vquestion(str), documents(List) and answer(str)\n",
    "class State(MessagesState):\n",
    "    run_id: str\n",
    "    question: str\n",
    "    ground_truth: str\n",
    "    documents: List[Document]\n",
    "    answer: str\n",
    "    evaluation: Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RAG Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent should:\n",
    "- fetch relevant document chunks based on the user query\n",
    "- combine the retrieved documents and use them as context\n",
    "- invoke the LLM to generate a response\n",
    "- evaluate the pipeline based on the ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    question = state[\"question\"]\n",
    "    retrieved_docs = vector_store.similarity_search(question)\n",
    "    return {\"documents\": retrieved_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(state: State):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "    template = ChatPromptTemplate([\n",
    "        (\"system\", \"You are an assistant for question-answering tasks.\"),\n",
    "        (\"human\", \"Use the following pieces of retrieved context to answer the question. \"\n",
    "                \"If you don't know the answer, just say that you don't know. \" \n",
    "                \"Use three sentences maximum and keep the answer concise. \"\n",
    "                \"\\n# Question: \\n-> {question} \"\n",
    "                \"\\n# Context: \\n-> {context} \"\n",
    "                \"\\n# Answer: \"),\n",
    "    ])\n",
    "\n",
    "    messages = template.invoke(\n",
    "        {\"context\": docs_content, \"question\": question}\n",
    "    ).to_messages()\n",
    "\n",
    "    return {\"messages\": messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state: State):\n",
    "    ai_message = llm.invoke(state[\"messages\"])\n",
    "    return {\"answer\": ai_message.content, \"messages\": ai_message}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag(state: State):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    answer = state[\"answer\"]\n",
    "    ground_truth = state[\"ground_truth\"]\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            \"question\": [question],\n",
    "            \"answer\": [answer],\n",
    "            \"contexts\": [[doc.page_content for doc in documents]],\n",
    "            \"ground_truth\": [ground_truth]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    evaluation_results = evaluate(\n",
    "        dataset=dataset,\n",
    "        llm=llm_judge\n",
    "    )\n",
    "    print(evaluation_results)\n",
    "\n",
    "    # TODO - Log metrics in MLflow\n",
    "    # The evaluation_results output value is a list\n",
    "    # Example: evaluation_results[\"faithfulness\"][0]\n",
    "    with mlflow.start_run(state[\"run_id\"]):\n",
    "        log_metrics({\n",
    "                \"faithfulness\": evaluation_results[\"faithfulness\"],\n",
    "                \"context_precision\": evaluation_results[\"context_precision\"],\n",
    "                \"context_recall\": evaluation_results[\"context_recall\"],\n",
    "                \"answer_relevancy\": evaluation_results[\"answer_relevancy\"],\n",
    "\n",
    "        })\n",
    "\n",
    "    return {\"evaluation\": evaluation_results}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build the LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x139215fd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "     # TODO - add all the nodes and edges\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"augment\", augment)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"evaluate\", evaluate_rag)\n",
    "\n",
    "# Add edges to define the flow\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"augment\")\n",
    "workflow.add_edge(\"augment\", \"generate\")\n",
    "workflow.add_edge(\"generate\", \"evaluate\")\n",
    "workflow.add_edge(\"evaluate\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = workflow.compile()\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Invoke the Workflow with a Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = [\n",
    "    {\n",
    "        \"question\": \"What are Open source models?\",\n",
    "        \"ground_truth\": \"Open-source models are AI or machine learning \" \n",
    "                        \"models whose code, architecture, and in some cases, \" \n",
    "                        \"training data and weights, are publicly available for \" \n",
    "                        \"use, modification, and distribution. They enable \" \n",
    "                        \"collaboration, transparency, and innovation by allowing \" \n",
    "                        \"developers to fine-tune, deploy, or improve them without \" \n",
    "                        \"proprietary restrictions.\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m output = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mground_truth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mground_truth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlflow_run_id\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/sandbox-udacity-ai-agents/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3094\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3091\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3092\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3094\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3095\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3096\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3097\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3098\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3099\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3100\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3102\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3103\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3104\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3106\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3107\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3108\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3109\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/sandbox-udacity-ai-agents/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2679\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2677\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2678\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2679\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2680\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2684\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2685\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2686\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2687\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2688\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2689\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/sandbox-udacity-ai-agents/.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/sandbox-udacity-ai-agents/.venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/sandbox-udacity-ai-agents/.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/sandbox-udacity-ai-agents/.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mevaluate_rag\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      5\u001b[39m ground_truth = state[\u001b[33m\"\u001b[39m\u001b[33mground_truth\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      6\u001b[39m dataset = Dataset.from_dict(\n\u001b[32m      7\u001b[39m     {\n\u001b[32m      8\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: [question],\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     }\n\u001b[32m     13\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m evaluation_results = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm_judge\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(evaluation_results)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# TODO - Log metrics in MLflow\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# The evaluation_results output value is a list\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Example: evaluation_results[\"faithfulness\"][0]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/sandbox-udacity-ai-agents/.venv/lib/python3.13/site-packages/ragas/_analytics.py:277\u001b[39m, in \u001b[36mtrack_was_completed.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    276\u001b[39m     track(IsCompleteEvent(event_type=func.\u001b[34m__name__\u001b[39m, is_completed=\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m     track(IsCompleteEvent(event_type=func.\u001b[34m__name__\u001b[39m, is_completed=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/sandbox-udacity-ai-agents/.venv/lib/python3.13/site-packages/ragas/evaluation.py:461\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(dataset, metrics, llm, embeddings, experiment_name, callbacks, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar, return_executor, allow_nest_asyncio)\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m# Default behavior: use nest_asyncio for backward compatibility (Jupyter notebooks)\u001b[39;00m\n\u001b[32m    459\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01masync_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_async_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/sandbox-udacity-ai-agents/.venv/lib/python3.13/site-packages/ragas/async_utils.py:156\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(async_func, allow_nest_asyncio)\u001b[39m\n\u001b[32m    148\u001b[39m     loop_type = \u001b[38;5;28mtype\u001b[39m(loop).\u001b[34m__name__\u001b[39m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    150\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot execute nested async code with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloop_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    151\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33muvloop does not support nested event loop execution. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    152\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease use asyncio\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms standard event loop in Jupyter environments, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    153\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mor refactor your code to avoid nested async calls.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    154\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoro\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/sandbox-udacity-ai-agents/.venv/lib/python3.13/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/sandbox-udacity-ai-agents/.venv/lib/python3.13/site-packages/nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.7-macos-aarch64-none/lib/python3.13/asyncio/futures.py:199\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.7-macos-aarch64-none/lib/python3.13/asyncio/tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/sandbox-udacity-ai-agents/.venv/lib/python3.13/site-packages/ragas/evaluation.py:434\u001b[39m, in \u001b[36mevaluate.<locals>._async_wrapper\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_async_wrapper\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m aevaluate(\n\u001b[32m    435\u001b[39m         dataset=dataset,\n\u001b[32m    436\u001b[39m         metrics=metrics,\n\u001b[32m    437\u001b[39m         llm=llm,\n\u001b[32m    438\u001b[39m         embeddings=embeddings,\n\u001b[32m    439\u001b[39m         experiment_name=experiment_name,\n\u001b[32m    440\u001b[39m         callbacks=callbacks,\n\u001b[32m    441\u001b[39m         run_config=run_config,\n\u001b[32m    442\u001b[39m         token_usage_parser=token_usage_parser,\n\u001b[32m    443\u001b[39m         raise_exceptions=raise_exceptions,\n\u001b[32m    444\u001b[39m         column_map=column_map,\n\u001b[32m    445\u001b[39m         show_progress=show_progress,\n\u001b[32m    446\u001b[39m         batch_size=batch_size,\n\u001b[32m    447\u001b[39m         _run_id=_run_id,\n\u001b[32m    448\u001b[39m         _pbar=_pbar,\n\u001b[32m    449\u001b[39m         return_executor=return_executor,\n\u001b[32m    450\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/sandbox-udacity-ai-agents/.venv/lib/python3.13/site-packages/ragas/evaluation.py:176\u001b[39m, in \u001b[36maevaluate\u001b[39m\u001b[34m(dataset, metrics, llm, embeddings, experiment_name, callbacks, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar, return_executor)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(metric, MetricWithEmbeddings) \u001b[38;5;129;01mand\u001b[39;00m metric.embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m         embeddings = \u001b[43membedding_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m     metric.embeddings = embeddings\n\u001b[32m    178\u001b[39m     embeddings_changed.append(i)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/sandbox-udacity-ai-agents/.venv/lib/python3.13/site-packages/ragas/embeddings/base.py:664\u001b[39m, in \u001b[36membedding_factory\u001b[39m\u001b[34m(provider, model, run_config, client, interface, base_url, **kwargs)\u001b[39m\n\u001b[32m    658\u001b[39m \u001b[38;5;66;03m# Legacy interface - treat provider as model name if it looks like a model\u001b[39;00m\n\u001b[32m    659\u001b[39m model_name = (\n\u001b[32m    660\u001b[39m     provider\n\u001b[32m    661\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _looks_like_model_name(provider)\n\u001b[32m    662\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m (model \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtext-embedding-ada-002\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    663\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m664\u001b[39m openai_embeddings = \u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    666\u001b[39m     openai_embeddings.request_timeout = run_config.timeout\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/sandbox-udacity-ai-agents/.venv/lib/python3.13/site-packages/langchain_openai/embeddings/base.py:366\u001b[39m, in \u001b[36mOpenAIEmbeddings.validate_environment\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    361\u001b[39m         \u001b[38;5;28mself\u001b[39m.http_async_client = httpx.AsyncClient(proxy=\u001b[38;5;28mself\u001b[39m.openai_proxy)\n\u001b[32m    362\u001b[39m     async_specific = {\n\u001b[32m    363\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttp_client\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.http_async_client,\n\u001b[32m    364\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mapi_key\u001b[39m\u001b[33m\"\u001b[39m: async_api_key_value,\n\u001b[32m    365\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m     \u001b[38;5;28mself\u001b[39m.async_client = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAsyncOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43masync_specific\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.embeddings\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/sandbox-udacity-ai-agents/.venv/lib/python3.13/site-packages/openai/_client.py:488\u001b[39m, in \u001b[36mAsyncOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    486\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    489\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    490\u001b[39m     )\n\u001b[32m    491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(api_key):\n\u001b[32m    492\u001b[39m     \u001b[38;5;28mself\u001b[39m.api_key = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
      "During task with name 'evaluate' and id '939bebb7-8b5e-7cc1-6068-80e82db28e39'"
     ]
    }
   ],
   "source": [
    "output = graph.invoke(\n",
    "    {\n",
    "        \"question\": reference[0][\"question\"],\n",
    "        \"ground_truth\": reference[0][\"ground_truth\"],\n",
    "        \"run_id\": mlflow_run_id\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inspect in MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Get MLFlow Run with .get_run()\n",
    "mflow_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you understood how it works, experiment with new things.\n",
    "\n",
    "- Change RAG parameters: embedding model, chunk_size, chunk_overlap...\n",
    "- Create multiple runs\n",
    "- Improve your reference with more questions and ground_truth answers\n",
    "- Use the results to understand what are the best parameters\n",
    "- Create an Agent that picks the best combination"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
